{"cells":[{"cell_type":"markdown","source":["SETUP"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8ae47336-d99d-448d-bc03-d6d8d3004673"},{"cell_type":"code","source":["#first let's install needed packages\n","\n","%pip install semantic-link-labs pyapacheatlas"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a812563-9084-4466-b916-3d28b90feff4"},{"cell_type":"code","source":["import sempy_labs as labs\n","import sempy_labs.report as rep\n","from sempy_labs.report import ReportWrapper"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.2999095Z","session_start_time":null,"execution_start_time":"2024-11-12T19:52:26.2806426Z","execution_finish_time":"2024-11-12T19:52:38.3682641Z","parent_msg_id":"6be6e4ab-009e-4ce6-918e-a4b3ab615841"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1638b317-68a6-4be6-affb-72ce4c5a45fe"},{"cell_type":"code","source":["import pyodbc"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.300523Z","session_start_time":null,"execution_start_time":"2024-11-12T19:52:38.8304888Z","execution_finish_time":"2024-11-12T19:52:39.087237Z","parent_msg_id":"6a9c82b7-2d91-426d-b65f-cc946dc9fe5e"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ba24a0f-c14a-40a0-a5c0-1a2fa2c79e04"},{"cell_type":"markdown","source":["AUTHENTICATE SERVICE PRINCIPAL FOR FABRIC\n","It must have \"_itemType_.ReadAll\" and \"_itemType_.ReadWriteAll\" for Tables, DataPipelines, Workspaces, etc. that we want to scan"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16d26399-b89a-4bbc-adae-4d0104a62dc1"},{"cell_type":"code","source":["tenant_id=\"\"\n","client_id=\"\"\n","client_secret=\"\"\n","PurviewAccount_name=\"\"\n","fabric_SQL_connection_string=\"\"\n","\n","\n","import requests\n","base_url_auth = 'https://login.microsoftonline.com'\n","relative_url_auth=f'/{tenant_id}/oauth2/v2.0/token'\n","url_auth= f'{base_url_auth}/{relative_url_auth}'\n","data_auth = f'client_id={client_id}\\n&client_secret={client_secret}\\n&grant_type=client_credentials\\n&scope=https://api.fabric.microsoft.com/.default'\n","header_auth={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n","print(\"url_auth=\",url_auth)\n","print(\"data_auth=\",data_auth)\n","print(\"header_auth=\",header_auth)\n","\n","response = requests.post(url_auth, data = data_auth, headers=header_auth)\n","\n","print(response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7ad06465-4b80-4abb-9262-c3f4fa8e9e27"},{"cell_type":"code","source":["#extract auth token\n","import json\n","#print(response.text)\n","parsable=json.loads(response.text)\n","#auth_token=parsable[\"access_token\"]\n","auth_token=response.json().get('access_token')\n","#print(\"1=|\",auth_token,\"|-\")\n","\n","\n","#or you can use\n","# print(json.loads(response.text)[\"access_token\"])\n","bearer=f'Bearer {auth_token}'\n","print(bearer)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25e5824b-fa85-4fa0-83c8-c3b0b9254a22"},{"cell_type":"code","source":["import pyodbc\n","#adding a comment here\n","\n","service_principal_id = f\"{client_id}@{tenant_id}\" # this is a very important pattern client_id@tenant_id\n","\n","odbc_connection_string_template= (\n","    f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n","    f\"SERVER={fabric_SQL_connection_string};\"\n","    f\"UID={service_principal_id};\"\n","    f\"PWD={client_secret};\"\n","    f\"Authentication=ActiveDirectoryServicePrincipal;\"\n",")\n","# the \"DATABASE={database_name}\" part will be added later\n","\n","query_tables='SELECT name, object_id, create_date, modify_date, type, type_desc from sys.tables'\n","query_columns_template1=(\n","    f\"SELECT c.name AS column_name \"  \n","    f\",c.column_id \"  \n","    f\",SCHEMA_NAME(t.schema_id) AS type_schema \"  \n","    f\",t.name AS type_name \"  \n","    f\",t.is_user_defined \"  \n","    f\",t.is_assembly_type \"  \n","    f\",c.max_length \"  \n","    f\",c.precision \"  \n","    f\",c.scale \"  \n","    f\"FROM sys.columns AS c \"\n","    f\"JOIN sys.types AS t ON c.user_type_id=t.user_type_id \"  \n","    f\"WHERE c.object_id = OBJECT_ID('\"\n",")\n","query_columns_template2=\"') ORDER BY c.column_id\" "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.3043892Z","session_start_time":null,"execution_start_time":"2024-11-12T19:52:41.842776Z","execution_finish_time":"2024-11-12T19:52:42.0757244Z","parent_msg_id":"9add4488-37a6-40b5-b0f1-671353449fa4"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21cc7830-507a-4ff2-961e-81d09c8a86f1"},{"cell_type":"code","source":["# create some container dataframes to hold the metadata we gather\n","\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","# record template for table columns metadata\n","df_columns_datarow={\"WorkspaceName\": \"dummy_column1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"ColumnType\": \"\", \\\n","        \"ColumnID\":\"\"}\n","\n","schema_df_columns = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"ColumnType\", StringType(), True), \\\n","    StructField(\"ColumnID\", StringType(), True) \\\n","  ])\n"," \n","df_columns = spark.createDataFrame(data=[],schema=schema_df_columns)\n","df_columns.show()\n","\n","# record template for table metadata\n","#table type in this df is for tables/views/storedprocs...\n","df_tables_datarow={\"WorkspaceName\": \"dummy_table_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableType\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n"," \n","\n","schema_df_tables = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","    ])\n"," \n","df_tables = spark.createDataFrame(data=[],schema=schema_df_tables)\n","df_tables.show()\n","\n","# record template for report metadata\n","# source can be a table or a view\n","df_reports_datarow={\"WorkspaceName\": \"dummy_report_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"ReportName\": \"\", \\\n","        \"ReportID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceID\":\"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_reports = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"ReportName\",StringType(),True), \\\n","    StructField(\"ReportID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"SourceName\", StringType(), True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_reports = spark.createDataFrame(data=[],schema=schema_df_reports)\n","df_reports.show()\n","\n","# record template for DataPipelines metadata\n","# source can be a table or a view\n","df_lineage_datarow={\"WorkspaceName\": \"dummy_lineage_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"PipelineID\": \"\", \\\n","        \"PipelineName\": \"\", \\\n","        \"ActivityName\": \"\", \\\n","        \"ActivityType\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceContainerName\": \"\", \\\n","        \"SourceContainerID\": \"\", \\\n","        \"SinkType\": \"\", \\\n","        \"SinkName\": \"\", \\\n","        \"SinkContainer Name\": \"\", \\\n","        \"SinkContainer ID\": \"\", \\\n","        \"ColumnMappings\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_lineage = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"PipelineID\",StringType(),True), \\\n","    StructField(\"PipelineName\",StringType(),True), \\\n","    StructField(\"ActivityName\", StringType(), True), \\\n","    StructField(\"ActivityType\", StringType(), True), \\\n","    StructField(\"SourceName\",StringType(),True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceContainerName\", StringType(), True), \\\n","    StructField(\"SourceContainerID\", StringType(), True), \\\n","    StructField(\"SinkName\",StringType(),True), \\\n","    StructField(\"SinkType\", StringType(), True), \\\n","    StructField(\"SinkContainerName\", StringType(), True), \\\n","    StructField(\"SinkContainerID\", StringType(), True), \\\n","    StructField(\"ColumnMappings\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_lineage = spark.createDataFrame(data=[],schema=schema_df_lineage)\n","df_lineage.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.3055171Z","session_start_time":null,"execution_start_time":"2024-11-12T19:52:42.5257616Z","execution_finish_time":"2024-11-12T19:52:46.2490081Z","parent_msg_id":"f887f501-96c3-4717-abf2-aef9718329be"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+----------+----------+--------+\n|WorkspaceName|WorkspaceID|WarehouseName|WarehouseID|LakehouseName|LakehouseID|TableName|TableID|ColumnName|ColumnType|ColumnID|\n+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+----------+----------+--------+\n+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+----------+----------+--------+\n\n+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+-----------+----------+\n|WorkspaceName|WorkspaceID|WarehouseName|WarehouseID|LakehouseName|LakehouseID|TableName|TableID|PurviewGUID|PurviewFQN|\n+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+-----------+----------+\n+-------------+-----------+-------------+-----------+-------------+-----------+---------+-------+-----------+----------+\n\n+-------------+-----------+----------+--------+----------+----------+----------+--------+-----------+----------+\n|WorkspaceName|WorkspaceID|ReportName|ReportID|ColumnName|SourceName|SourceType|SourceID|PurviewGUID|PurviewFQN|\n+-------------+-----------+----------+--------+----------+----------+----------+--------+-----------+----------+\n+-------------+-----------+----------+--------+----------+----------+----------+--------+-----------+----------+\n\n+-------------+-----------+----------+------------+------------+------------+----------+----------+-------------------+-----------------+--------+--------+-----------------+---------------+--------------+-----------+----------+\n|WorkspaceName|WorkspaceID|PipelineID|PipelineName|ActivityName|ActivityType|SourceName|SourceType|SourceContainerName|SourceContainerID|SinkName|SinkType|SinkContainerName|SinkContainerID|ColumnMappings|PurviewGUID|PurviewFQN|\n+-------------+-----------+----------+------------+------------+------------+----------+----------+-------------------+-----------------+--------+--------+-----------------+---------------+--------------+-----------+----------+\n+-------------+-----------+----------+------------+------------+------------+----------+----------+-------------------+-----------------+--------+--------+-----------------+---------------+--------------+-----------+----------+\n\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"012cc47d-d101-4363-a7ac-20b1ea271faa"},{"cell_type":"markdown","source":["EXTRACT ALL WORKSPACES"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e2802-d322-4e3c-8e44-e1cc9583b09a"},{"cell_type":"code","source":["#get workspaces\n","get_workspaces_url = f'https://api.fabric.microsoft.com/v1/workspaces'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","data_dummy={\"dummy\":\"true\"}\n","#print(\"header_token=\", header_token)\n","workspaces_response=requests.get(get_workspaces_url,headers=header_token, data=data_dummy)\n","print(\"workspaces_response\",workspaces_response.text)\n","\n","\n","wkspaces=json.loads(workspaces_response.text)[\"value\"]\n","for wkspc in wkspaces:\n","    current_wkspace_id=wkspc[\"id\"]\n","    current_wkspace_name=wkspc[\"displayName\"]\n","    print(f'Processing workspace:  {current_wkspace_name} ({current_wkspace_id})\\n')\n","    \n","    new_row = df_tables_datarow.copy()\n","    new_row.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id})\n","    df_tables = df_tables.union(spark.createDataFrame(data=[new_row],schema=schema_df_tables))\n","    df_tables.show()\n","    \n","\n","    get_items_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/items'\n","    items_response=requests.get(get_items_url,headers=header_token, data=data_dummy)\n","    #print(items_response.text)\n","  \n","    items=json.loads(items_response.text)[\"value\"]\n","    for item in items:\n","      item_name=item[\"displayName\"]\n","      item_type=item[\"type\"]\n","      item_id=item[\"id\"]\n","      print(f'\\n{item_name} is of type {item_type} and has id={item[\"id\"]}')\n","      if item_type == \"Warehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          odbc_conn_string=f'{odbc_connection_string_template}DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute a query\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              #print(\"Found table: \",row)\n","              table_name=row[0]\n","\n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"WarehouseName\": f\"{database_name}\", \\\n","              \"WarehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"WarehouseName\": f\"{database_name}\", \\\n","                \"WarehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","              \n","          #per chiudere bene\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Lakehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          odbc_conn_string=f'{odbc_connection_string_template}DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute a query\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              #print(\"Found table: \",row)\n","              table_name=row[0]\n","              \n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"LakehouseName\": f\"{database_name}\", \\\n","              \"LakehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"LakehouseName\": f\"{database_name}\", \\\n","                \"LakehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","          #per chiudere bene\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Report\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit for reports\n","          try:\n","                rpt = ReportWrapper(report=item_name, workspace=current_wkspace_name)\n","                pandadf_report_lineage=rpt.list_semantic_model_objects()\n","                #print(pandadf_report_lineage)\n","                for row in pandadf_report_lineage.itertuples():\n","                    #print(\"pandas row=\",row)\n","                    #print(row[2])\n","                    if row[3] == \"Column\":\n","                        #aggiungi riga lineage con campo row[2] e tabella/vista row[1]\n","                        # save report lineage item in dataframe for reports\n","                        new_dict=df_reports_datarow.copy()\n","                        new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                        \"ReportName\": f'{item_name}', \\\n","                        \"ReportID\": f'{item_id}', \\\n","                        \"ColumnName\": f'{row[2]}', \\\n","                        \"SourceName\": f'{row[1]}', \\\n","                        #\"Source ID\": f'{column[1]}', \\\n","                        \"SourceType\": \"Table\"}) #only tables so far\n","                        #print(f'new_dict={new_dict}')\n","                        new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_reports)\n","                        #print(f'new_dict={new_row}')\n","                        df_reports = df_reports.union(new_row)\n","                        #todo cerca sources, cerca report sink,crea lineage link\n","          except ValueError as verr:\n","            print(\"report not valid\")\n","            print(\"error=\",verr)\n","\n","                \n","     \n","      if item_type == \"DataPipeline\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit for pipelines\n","          pipeline_definition=labs.get_data_pipeline_definition(item_name,current_wkspace_id,decode=True)\n","          # check which type, and if we can manage this kind of pipeline\n","          # only pipelines with copy activity are managed so far\n","          all_activities=json.loads(pipeline_definition)[\"properties\"][\"activities\"]\n","          try:\n","                for activity in all_activities:\n","                        #print(\"activity \",activity)\n","                        activity_type=activity[\"type\"]\n","                        if activity_type == \"Copy\":\n","                            #gather metadata\n","                            activity_name=activity[\"name\"]\n","                            source_type = activity[\"typeProperties\"][\"source\"][\"type\"]\n","                            source_container=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            source_container_id=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            source_name=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            sink_type=activity[\"typeProperties\"][\"sink\"][\"type\"]\n","                            sink_container=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            sink_container_id=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            sink_name=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            source_sink_mappings=activity[\"typeProperties\"][\"translator\"][\"mappings\"]\n","                            mappings_list=[]\n","                            for mapping_item in source_sink_mappings:\n","                                mappings_list.append({\"Source\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"type\"]} {mapping_item[\"source\"][\"physicalType\"]})', \\\n","                                \"Sink\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"physicalType\"]})'})\n","                            mappings_string=json.dumps(mappings_list)\n","                            #print(\"mappings to string\",mappings_string)\n","                            # save metadata in dataframe\n","                            new_dict=df_lineage_datarow.copy()\n","                            new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                            \"PipelineName\": item_name, \\\n","                            \"PipelineID\": item_id, \\\n","                            \"ActivityName\": activity_name, \\\n","                            \"ActivityType\": activity_type, \\\n","                            \"SourceName\": source_name, \\\n","                            \"SourceType\": source_type, \\\n","                            \"SourceContainerName\": source_container, \\\n","                            \"SourceContainerID\": source_container_id, \\\n","                            \"SinkType\": sink_type, \\\n","                            \"SinkName\": sink_name, \\\n","                            \"SinkContainerName\": sink_container, \\\n","                            \"SinkContainerID\": sink_container_id, \\\n","                            \"ColumnMappings\": f'{mappings_string}', \\\n","                            \"PurviewFQN\":\"\"})\n","                            \n","                            #print(f'new_dict={new_dict}')\n","                            new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_lineage)\n","                            #print(f'new_dict={new_row}')\n","                            df_lineage = df_lineage.union(new_row)\n","                            break\n","          except KeyError as keyerr:\n","            print(\"Error parsing DataPipelines definition: \", keyerr)\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"error details \", keyerr)\n","          except NameError as namerr:\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"Error parsing DataPipelines definition: \", namerr)\n","                    \n","\n","              \n","              \n","print(\"Recap of Metadata found:\")\n","\n","print(\"Found Tables (includes partial rows for Lakehouses and Warehouses):\")\n","display(df_tables)\n","#df_tables.show()\n","\n","print(\"\\n\\nFound Table columns (for all tables) found:\")\n","#df_columns.show()\n","display(df_columns)\n","\n","print(\"\\n\\nFound PBIR Reports lineage to tables/views:\")\n","display(df_reports)\n","#df_reports.show()\n","\n","print(\"\\n\\nFound Copy DataPipelines, with lineage to tables/views:\")\n","display(df_lineage)\n","#df_lineage.show()\n","\n","print(\"\\nMetadata Extraction Completed.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f3709664-6259-4004-97b2-f0efb7fd7c50"},{"cell_type":"code","source":["import json\n","import pyapacheatlas\n","# PyApacheAtlas packages\n","# Connect to Atlas via a Service Principal\n","from pyapacheatlas.auth import ServicePrincipalAuthentication\n","from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess\n","from pyapacheatlas.core.typedef import EntityTypeDef, AtlasAttributeDef\n","from pyapacheatlas.core.util import GuidTracker\n","\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, when\n","import pyspark.sql"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.4622636Z","session_start_time":null,"execution_start_time":"2024-11-12T19:53:28.8001633Z","execution_finish_time":"2024-11-12T19:53:29.5924958Z","parent_msg_id":"31528387-c8a3-4e52-98c5-0a19cb45a1c7"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4465fa93-6736-46e4-ae7f-11856f3225c6"},{"cell_type":"code","source":["# here we enter our Service principal id and secret to access the MS Purview account APIs\n","oauth = ServicePrincipalAuthentication(\n","        tenant_id=\"\",\n","        client_id=\"\",\n","        client_secret=\"\"\n","    )\n","client = PurviewClient(\n","        account_name=\"<enter Purview account name here>\",\n","        authentication=oauth\n","    )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"42a15007-6029-45c3-9ec5-9a582d34e290","normalized_state":"finished","queued_time":"2024-11-12T19:51:41.4634924Z","session_start_time":null,"execution_start_time":"2024-11-12T19:53:30.0104913Z","execution_finish_time":"2024-11-12T19:53:30.4017265Z","parent_msg_id":"505fc0ef-ce86-47ff-9a4d-008dca479d61"},"text/plain":"StatementMeta(, 42a15007-6029-45c3-9ec5-9a582d34e290, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70fd7857-43db-446f-a95a-e612e89edc87"},{"cell_type":"code","source":[" # We need a custom process entity type that contains the definition for\n","    # a columnMapping attribute.\n","# THIS SHOULD ONLY BE RUN ONCE\n","procType = EntityTypeDef(\n","    \"Lineage_Extractor_Process\",\n","    superTypes=[\"Process\"],\n","    attributeDefs = [\n","        AtlasAttributeDef(\"columnMapping\")\n","    ]\n",")\n","\n","# Upload the type definition\n","type_results = client.upload_typedefs(entityDefs=[procType], force_update=True)\n","print(json.dumps(type_results,indent=2))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"57604287-caa9-4c8c-9cdb-8f43d9ec21b7"},{"cell_type":"markdown","source":["REGISTER TABLES"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71999375-cd28-4380-8179-0268dce09cd6"},{"cell_type":"code","source":["#new df where we will update the Purview fullyQualifiedName and GUID\n","df_tables_registered=spark.createDataFrame(data=[],schema=schema_df_tables)\n","\n","created_GUIDs=[]\n","# create Table nodes\n","gt = GuidTracker()\n","for table in df_tables.collect():\n","    table_dict=table.asDict()\n","    print(\"table_dict=\",table_dict)\n","    tablename=table_dict[\"TableName\"]\n","\n","    if tablename:\n","        if table_dict[\"WarehouseName\"]:\n","            artifactName=table_dict[\"WarehouseName\"]\n","        else:\n","            artifactName=table_dict[\"LakehouseName\"]\n","        table_entity = AtlasEntity(\n","            name=tablename,\n","            typeName=\"DataSet\",\n","            qualified_name=f'LineageExtractor://{table_dict[\"WorkspaceName\"]}/{artifactName}/{table_dict[\"TableName\"]}',\n","            guid=gt.get_guid(),\n","            attributes={\n","                \"userDescription\": f'<div>Fabric Table<p> contained within {artifactName}</p>'\n","            },\n","            # This custom attribute flips a switch inside of the Purview UI to render\n","            # the rich text description.\n","            customAttributes={\n","                \"microsoft_isDescriptionRichText\": \"true\"\n","            }\n","        )\n","        #print(table_entity)\n","        #print(table_entity.guid)\n","        #print(table_entity.qualifiedName)\n","        results = client.upload_entities([table_entity])\n","        \n","        table_dict[\"PurviewGUID\"] = results[\"guidAssignments\"][str(table_entity.guid)]    \n","        table_dict[\"PurviewFQN\"] = str(table_entity.qualifiedName)\n","        #print(f'new guid, new fqn={table_dict[\"PurviewGUID\"]}, {table_dict[\"PurviewFQN\"]}')\n","        \n","        created_GUIDs.append(table_dict[\"PurviewGUID\"])\n","\n","        new_dict=df_tables_datarow.copy()\n","        for key in table_dict.keys():\n","            #print(\"key=\",key)\n","            new_dict[key]=table_dict[key]\n","        #print(\"new_dict=\",new_dict)\n","        new_row=spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","        \n","        df_tables_registered=df_tables_registered.union(new_row)\n","          \n","        #df_tables.union(table)\n","        #df_tables=df_tables.withColumn(\"PurviewGUID\", when(col(\"TableName\") == tablename, createdPurviewGUID))\n","        #df_tables=df_tables.withColumn(\"PurviewFQN\", when(col(\"TableName\") == tablename, createdPurviewFQN))\n","\n","display(df_tables_registered)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b54b3937-e95f-4623-8bf9-3df0847f2d37"},{"cell_type":"markdown","source":["REGISTER REPORTS AND LINEAGE"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a2b446ff-5da1-4c04-9211-c7e857946ba3"},{"cell_type":"code","source":["#new df where we will update the Purview fullyQualifiedName and GUID\n","df_reports_registered=df_reports.alias(\"df_reports_registered\")\n","#errato perch√® genera una copia vuota\n","#df_reports_registered=spark.createDataFrame(data=[],schema=schema_df_reports)\n","\n","\n","gt = GuidTracker()\n","created_GUIDs=[]\n","# create Reports nodes and lineage\n","#first find all detected reports IDs\n","report_distinctIds=df_reports.select('ReportID').distinct().collect()\n","print(\"\\nreport_distinctIds:\")\n","display(report_distinctIds)\n","print()\n","\n","\n","for report_id in report_distinctIds:\n","    rep_id_dict=report_id.asDict()\n","    print(\"\\nrep_id_dict\",rep_id_dict)\n","    current_report_id=rep_id_dict[\"ReportID\"]\n","    report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","    report_sources=report_rows.collect()\n","    print(\"report sources=\",report_sources)\n","    print()\n","\n","    #for each report register it, and create lineage with tables\n","    #first register if if not already scanned by Purview\n","    report_metadata=report_sources[0].asDict()\n","    report_guid=report_metadata[\"PurviewGUID\"]\n","    report_fqn=report_metadata[\"PurviewFQN\"]\n","    if report_guid == \"\":\n","        report_entity = AtlasEntity(\n","            name=report_metadata[\"ReportName\"],\n","            typeName=\"DataSet\",\n","            qualified_name=f'LineageExtractor://{report_metadata[\"WorkspaceName\"]}/{report_metadata[\"ReportName\"]}_{current_report_id}',\n","            guid=gt.get_guid(),\n","            attributes={\n","                \"userDescription\": f'<div>Fabric Report<p>in PBIR format</p><p>contained within {report_metadata[\"WorkspaceName\"]}</p><p>Fabric ID is {current_report_id}</p>'\n","            },\n","            # This custom attribute flips a switch inside of the Purview UI to render\n","            # the rich text description.\n","            customAttributes={\n","                \"microsoft_isDescriptionRichText\": \"true\"\n","            }\n","        )\n","        print(report_entity)\n","        print(report_entity.guid)\n","        print(report_entity.qualifiedName)\n","        results = client.upload_entities([report_entity])\n","        print(\"\\nrisultato upload=\",json.dumps(results))\n","\n","        report_guid = results[\"guidAssignments\"][str(report_entity.guid)]    \n","        report_fqn = str(report_entity.qualifiedName)\n","        print(f'\\nnew guid, new fqn={report_guid}, {report_fqn}')\n","        \n","        created_GUIDs.append(report_guid)\n","        #change all row occurrences of the report by updating FQN and GUID\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewGUID\", when(col(\"ReportID\") == current_report_id, report_guid).otherwise(col(\"PurviewGUID\")))\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewFQN\", when(col(\"ReportID\") == current_report_id, report_fqn).otherwise(col(\"PurviewFQN\")))       \n","        #refresh report_rows\n","        report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","        \n","    #bisogna fare join tra nome tabella/vista sorgente qui nel df dei reports e in quello delle tabelle, e per ogni riga registrare un lineage\n","    reports_short=report_rows.select('ReportID','SourceName',\"ColumnName\",\"PurviewFQN\").withColumnRenamed(\"PurviewFQN\",\"ReportFQN\")\n","    print(\"\\nreports_short:\")\n","    print(reports_short)\n","    display(reports_short)\n","    print()\n","\n","    lineage_links=reports_short.join(df_tables_registered.select(\"TableName\",\"PurviewFQN\"),reports_short[\"SourceName\"] == df_tables_registered[\"TableName\"],\"inner\").orderBy(\"TableName\").collect()\n","    print(\"\\nlinks=\",lineage_links)\n","    display(lineage_links)\n","\n","    # loop through source tables to create lineage clumns mappings\n","    column_mapping_total=[]\n","    column_mapping_partial=[]\n","    inputs=[]\n","    sourceFQN=\"\"\n","    sinkFQN=\"\"\n","    current_table=lineage_links[0][\"TableName\"]\n","    previous_table=current_table\n","    print(f'\\nprevious table={previous_table}, current table={current_table}')\n","    for link in lineage_links:\n","        link_dict=link.asDict()\n","        print(\"\\nlink_dict=\",link_dict)\n","        current_table=link_dict[\"TableName\"]\n","        if previous_table == current_table:\n","            # just add another mapping entry\n","            print(\"tabelle uguali\")\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","        else:\n","            # pack info for current table\n","            print(f'\\nprevious table={previous_table}, current table={current_table}')\n","            print(f'tabelle diverse')\n","\n","            column_mapping_total.append(\n","                # This object defines the column lineage between table souces and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","            column_mapping_partial=[]\n","            registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","            print(\"recuperata entity tabella=\",registered_table_entity)\n","            inputs.append(registered_table_entity)\n","            # take new values\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","            previous_table=current_table\n","    # process last one\n","    print(\"\\nultima\")\n","    registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","    print(\"recuperata entity tabella=\",registered_table_entity)\n","    inputs.append(registered_table_entity)\n","    # take new values\n","    columnname=link_dict[\"ColumnName\"]\n","    #column_mapping_partial.append({\"Source:\": columnname,\"Sink:\":\"Visual\"})\n","    \n","    column_mapping_total.append(\n","                # This object defines the column lineage between table souces and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","\n","    registered_report_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=report_fqn)[\"entities\"][0]\n","\n","    print('\\n\\n\\ncolumn_mapping_total=', json.dumps(column_mapping_total))\n","    lineage_link_obj = AtlasProcess(\n","        name=f'Report Lineage for {report_metadata[\"ReportName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'LineageExtractor://{report_metadata[\"WorkspaceName\"]}/{report_metadata[\"ReportName\"]}_{current_report_id}/ReportLineage',\n","        inputs=inputs,\n","        outputs=[registered_report_entity],\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric Report Mapping<p> contained within {report_metadata[\"WorkspaceName\"]}</p> <p> Report Id is {current_report_id}</p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    print(\"\\ncreated guids=\",created_GUIDs)\n","    print(\"\\nentity to be created=\",lineage_link_obj)\n","    results = client.upload_entities([lineage_link_obj])\n","    print(\"\\n upload result=\",json.dumps(results))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9fbbcf6d-0dac-48df-b0b7-dc19a2d39aa6"},{"cell_type":"code","source":["# debug tests\n","print(lineage_link_obj.attributes)\n","print()\n","display(df_reports_registered)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"16518500-e9a8-45ac-95ae-0c87e36fbb90"},{"cell_type":"code","source":["#TODO: move created GUIDs to different collection"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2024-11-12T19:51:41.485858Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-11-12T19:53:54.6147827Z","parent_msg_id":"bf44727e-810e-4ad6-b4b6-c86bf0e1aec4"},"text/plain":"StatementMeta(, , , Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b6be543e-093f-4e6c-9133-2aaccad5d7d8"},{"cell_type":"markdown","source":["REGISTER PIPELINES AND LINEAGE"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2be18311-7afa-4b6b-8046-d12f0ef67d89"},{"cell_type":"code","source":["df_pipelines_registered=df_lineage.alias(\"df_pipelines_registered\")\n","\n","\n","\n","gt = GuidTracker()\n","created_GUIDs=[]\n","# create Reports nodes and lineage\n","#first find all detected reports IDs\n","pipelines_list=df_pipelines_registered.collect()\n","print(\"\\npipelines_list:\")\n","display(pipelines_list)\n","print()\n","\n","\n","for pipeline in pipelines_list:\n","    source_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SourceName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SourceContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SourceContainerID\"]))).first()\n","    source_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=source_table[\"PurviewFQN\"])[\"entities\"][0]\n","    print(\"recuperata entity tabella source=\",source_table_entity)\n","    inputs=[source_table_entity]\n","\n","    sink_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SinkName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SinkContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SinkContainerID\"]))).first()\n","    sink_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sink_table[\"PurviewFQN\"])[\"entities\"][0]\n","    print(\"recuperata entity tabella sink =\",sink_table_entity)\n","    outputs=[sink_table_entity]\n","    \n","    column_mapping_partial=json.loads(pipeline[\"ColumnMappings\"])\n","    column_mapping_total=[{\"ColumnMapping\": column_mapping_partial,\n","        \"DatasetMapping\": {\"Source\": source_table[\"PurviewFQN\"], \"Sink\": sink_table[\"PurviewFQN\"]}\n","        }]\n","\n","\n","    pipeline_entity=AtlasProcess(\n","        name=f'DataPipeline {pipeline[\"PipelineName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'DataPipeline://{pipeline[\"WorkspaceName\"]}/{pipeline[\"PipelineName\"]}_{pipeline[\"PipelineID\"]}/CopyActivity',\n","        inputs=inputs,\n","        outputs=outputs,\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric DataPipeline Mapping<p> contained within {pipeline[\"WorkspaceName\"]}</p> <p> </p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    results = client.upload_entities([pipeline_entity])\n","    print(results)\n","    print(\"\\n upload result=\",json.dumps(results))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8e989797-cc57-4237-86c7-5b0ef5b720c7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}