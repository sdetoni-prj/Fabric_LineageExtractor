{"cells":[{"cell_type":"markdown","source":["# **SETUP**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8ae47336-d99d-448d-bc03-d6d8d3004673"},{"cell_type":"markdown","source":["This workbook does 4 things:\n","\n","1. first it provides some setup cells, to enable authentication and create storage dataframes for the results\n","\n","2. then it provides some examples of use of Semantic Link Lab, Scanner APIs, etc. for extracting Fabric metadata\n","\n","3. finally it explores all workspaces to which the provided user/service principal has access and extracts:\n","\n","    1. Tables with their columns (this uses a SQL query, therefore it can be performed also from a SQL client application external to Fabric)\n","\n","    2. Reports with their source tables (using Semantic Labs lib, therefore this can only be performed within Fabric, unless you leverage the low-level APIs which are used by SemanticLink labs)\n","\n","    3. Details of Copy pipelines with column-level mappings (the source json is extracted via Semantic Labs lib)\n","\n","4. eventually all metadata is uploaded into MS Purview Data Governance, and lineage links with column mappings are created for the discovered artifacts above "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07c84ced-5b15-4329-8a3d-be3de9666d2f"},{"cell_type":"code","source":["#first let's install needed packages\n","\n","%pip install semantic-link-labs pyapacheatlas"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a812563-9084-4466-b916-3d28b90feff4"},{"cell_type":"markdown","source":["ENTER your SERVICE PRINCIPAL credentials here, or better use an Azure Keyvault\n","\n","The clientid, clientsecret, tenantid variables are used to access one or more MS Fabric Workspaces\n","\n","The clientid4purview, clientsecret4purview, tenantid4purview variables are used to access a Purview Data Governance instance where the metadata (tables, columns, reports, pipelines) will be uploaded\n","\n","The demo_workspace and demo_workspace_id can be used for demo/test purposes in order to show/scan a single workspace"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b5dc715-6715-49a0-9e8d-7a2f32d0b9be"},{"cell_type":"code","source":["tenant_id=\"\"\n","tenantid4purview=tenant_id\n","client_id=\"\"\n","clientid4purview=client_id\n","client_secret=\"\"\n","clientsecret4purview=client_secret\n","PurviewAccount_name=\"\"\n","# you may want to limit extraction to just one workspace for a quick try. in this case uncomment lines 17 and 18 in cell [17] \"Extract all Workspaces Metadata\"\n","demo_workspace=\"\"\n","demo_workspace_id=\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:03.3253152Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:07.5673867Z","execution_finish_time":"2025-05-05T15:01:07.8930358Z","parent_msg_id":"bfceb822-c4b5-4edc-828a-8ad36a2e6faf"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70683a44-1ad6-41a1-9973-d05d6e1c74f3"},{"cell_type":"markdown","source":["First authenticate and create an access token"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd9bd368-3153-4f2a-9534-0982ea6e6198"},{"cell_type":"code","source":["import requests\n","base_url_auth = 'https://login.microsoftonline.com'\n","relative_url_auth=f'/{tenant_id}/oauth2/v2.0/token'\n","url_auth= f'{base_url_auth}/{relative_url_auth}'\n","data_auth = f'client_id={client_id}\\n&client_secret={client_secret}\\n&grant_type=client_credentials\\n&scope=https://api.fabric.microsoft.com/.default'\n","header_auth={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n","print(\"url_auth=\",url_auth)\n","print(\"data_auth=\",data_auth)\n","print(\"header_auth=\",header_auth)\n","\n","response = requests.post(url_auth, data = data_auth, headers=header_auth)\n","\n","#print(response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7ad06465-4b80-4abb-9262-c3f4fa8e9e27"},{"cell_type":"code","source":["import sempy_labs as labs\n","import sempy_labs.report as rep\n","from sempy_labs.report import ReportWrapper"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:33.9241793Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:33.9253139Z","execution_finish_time":"2025-05-05T15:01:36.2385009Z","parent_msg_id":"a2b7cca1-f189-43ba-9124-8fa09fcab75f"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1638b317-68a6-4be6-affb-72ce4c5a45fe"},{"cell_type":"code","source":["import pyodbc"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:38.5954032Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:38.5964975Z","execution_finish_time":"2025-05-05T15:01:38.9331033Z","parent_msg_id":"a0ee3052-1738-490c-8011-d974ea82d4be"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ba24a0f-c14a-40a0-a5c0-1a2fa2c79e04"},{"cell_type":"markdown","source":["AUTHENTICATE SERVICE PRINCIPAL FOR FABRIC\n","\n","The Svc Principal must not have \"_itemType_.ReadAll\" and \"_itemType_.ReadWriteAll\" for Tables, DataPipelines, Workspaces, etc. that we want to scan using Scanner APIs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16d26399-b89a-4bbc-adae-4d0104a62dc1"},{"cell_type":"code","source":["#extract auth token\n","import json\n","#print(response.text)\n","parsable=json.loads(response.text)\n","#auth_token=parsable[\"access_token\"]\n","auth_token=response.json().get('access_token')\n","#print(\"1=|\",auth_token,\"|-\")\n","\n","\n","#or you can use\n","# print(json.loads(response.text)[\"access_token\"])\n","bearer=f'Bearer {auth_token}'\n","# print(bearer)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:43.7919521Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:43.7929891Z","execution_finish_time":"2025-05-05T15:01:44.0950628Z","parent_msg_id":"33d096c0-1598-4f4f-bc9f-27317e59d2fd"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25e5824b-fa85-4fa0-83c8-c3b0b9254a22"},{"cell_type":"code","source":["import pyodbc\n","# these are the SQL queries used to scan tables and fields, they can be used also from an external SQL client \n","# instead of a notebook inside Fabric\n","\n","service_principal_id = f\"{client_id}@{tenant_id}\" # this is a very important pattern client_id@tenant_id\n","\n","odbc_connection_string_template= (\n","    f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n","    f\"UID={service_principal_id};\"\n","    f\"PWD={client_secret};\"\n","    f\"Authentication=ActiveDirectoryServicePrincipal;\"\n",")\n","# the \"DATABASE={database_name}\" and SERVER=\"{Fabric SQL connection string}\" part will be added later\n","\n","query_tables='SELECT name, object_id, create_date, modify_date, type, type_desc from sys.tables'\n","query_columns_template1=(\n","    f\"SELECT c.name AS column_name \"  \n","    f\",c.column_id \"  \n","    f\",SCHEMA_NAME(t.schema_id) AS type_schema \"  \n","    f\",t.name AS type_name \"  \n","    f\",t.is_user_defined \"  \n","    f\",t.is_assembly_type \"  \n","    f\",c.max_length \"  \n","    f\",c.precision \"  \n","    f\",c.scale \"  \n","    f\"FROM sys.columns AS c \"\n","    f\"JOIN sys.types AS t ON c.user_type_id=t.user_type_id \"  \n","    f\"WHERE c.object_id = OBJECT_ID('\"\n",")\n","query_columns_template2=\"') ORDER BY c.column_id\" "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:49.1937197Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:49.1948127Z","execution_finish_time":"2025-05-05T15:01:49.4993911Z","parent_msg_id":"5a53ac6e-9cdf-4b5f-8dfe-4cad9db9c8b4"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21cc7830-507a-4ff2-961e-81d09c8a86f1"},{"cell_type":"code","source":["# WE create some container dataframes to hold the metadata we gather\n","\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","# record template for table columns metadata\n","df_columns_datarow={\"WorkspaceName\": \"dummy_column1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"ColumnType\": \"\", \\\n","        \"ColumnID\":\"\"}\n","\n","schema_df_columns = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"ColumnType\", StringType(), True), \\\n","    StructField(\"ColumnID\", StringType(), True) \\\n","  ])\n"," \n","df_columns = spark.createDataFrame(data=[],schema=schema_df_columns)\n","#df_columns.show()\n","\n","# record template for table metadata\n","#table type in this df is for tables/views/storedprocs...\n","df_tables_datarow={\"WorkspaceName\": \"dummy_table_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"WarehouseName\": \"\", \\\n","        \"WarehouseID\": \"\", \\\n","        \"LakehouseName\": \"\", \\\n","        \"LakehouseID\": \"\", \\\n","        \"TableName\": \"\", \\\n","        \"TableType\": \"\", \\\n","        \"TableID\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n"," \n","\n","schema_df_tables = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"WarehouseName\",StringType(),True), \\\n","    StructField(\"WarehouseID\",StringType(),True), \\\n","    StructField(\"LakehouseName\",StringType(),True), \\\n","    StructField(\"LakehouseID\",StringType(),True), \\\n","    StructField(\"TableName\",StringType(),True), \\\n","    StructField(\"TableID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","    ])\n"," \n","df_tables = spark.createDataFrame(data=[],schema=schema_df_tables)\n","# df_tables.show()\n","\n","# record template for report metadata\n","# source can be a table or a view\n","df_reports_datarow={\"WorkspaceName\": \"dummy_report_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"ReportName\": \"\", \\\n","        \"ReportID\": \"\", \\\n","        \"ColumnName\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceID\":\"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_reports = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"ReportName\",StringType(),True), \\\n","    StructField(\"ReportID\", StringType(), True), \\\n","    StructField(\"ColumnName\", StringType(), True), \\\n","    StructField(\"SourceName\", StringType(), True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceID\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_reports = spark.createDataFrame(data=[],schema=schema_df_reports)\n","#df_reports.show()\n","\n","# record template for DataPipelines metadata\n","# source can be a table or a view\n","df_lineage_datarow={\"WorkspaceName\": \"dummy_lineage_1\", \"WorkspaceID\": \"dummy_id\", \\\n","        \"PipelineID\": \"\", \\\n","        \"PipelineName\": \"\", \\\n","        \"ActivityName\": \"\", \\\n","        \"ActivityType\": \"\", \\\n","        \"SourceName\": \"\", \\\n","        \"SourceType\": \"\", \\\n","        \"SourceContainerName\": \"\", \\\n","        \"SourceContainerID\": \"\", \\\n","        \"SinkType\": \"\", \\\n","        \"SinkName\": \"\", \\\n","        \"SinkContainer Name\": \"\", \\\n","        \"SinkContainer ID\": \"\", \\\n","        \"ColumnMappings\": \"\", \\\n","        \"PurviewGUID\":\"\", \\\n","        \"PurviewFQN\":\"\"}\n","\n","schema_df_lineage = StructType([ \\\n","    StructField(\"WorkspaceName\",StringType(),True), \\\n","    StructField(\"WorkspaceID\",StringType(),True), \\\n","    StructField(\"PipelineID\",StringType(),True), \\\n","    StructField(\"PipelineName\",StringType(),True), \\\n","    StructField(\"ActivityName\", StringType(), True), \\\n","    StructField(\"ActivityType\", StringType(), True), \\\n","    StructField(\"SourceName\",StringType(),True), \\\n","    StructField(\"SourceType\", StringType(), True), \\\n","    StructField(\"SourceContainerName\", StringType(), True), \\\n","    StructField(\"SourceContainerID\", StringType(), True), \\\n","    StructField(\"SinkName\",StringType(),True), \\\n","    StructField(\"SinkType\", StringType(), True), \\\n","    StructField(\"SinkContainerName\", StringType(), True), \\\n","    StructField(\"SinkContainerID\", StringType(), True), \\\n","    StructField(\"ColumnMappings\", StringType(), True), \\\n","    StructField(\"PurviewGUID\", StringType(), True), \\\n","    StructField(\"PurviewFQN\", StringType(), True)\n","  ])\n"," \n","df_lineage = spark.createDataFrame(data=[],schema=schema_df_lineage)\n","#df_lineage.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:01:55.3209567Z","session_start_time":null,"execution_start_time":"2025-05-05T15:01:55.3221732Z","execution_finish_time":"2025-05-05T15:01:56.1474418Z","parent_msg_id":"b49e1a68-a29d-479c-85ee-a35f3075f1e6"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"012cc47d-d101-4363-a7ac-20b1ea271faa"},{"cell_type":"markdown","source":["# **EXAMPLE: HOW TO USE SCANNER API TO EXTRACT METADATA FROM FABRIC**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d106cfc1-8c9b-42ab-a2fc-e425023cdba4"},{"cell_type":"code","source":["# EXTRACT LIST OF WORKSPACES ACCESSIBLE BY THE SERVICE PRINCIPAL\n","#\n","\n","#GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified\n","get_modified_wkspc_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","data_dummy={\"dummy\":\"true\"}\n","\n","response=requests.get(get_modified_wkspc_url,headers=header_token, data=data_dummy)\n","print(\"response=\",response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1e69c49-34dc-4a9f-8a36-2d5dfc0a106d"},{"cell_type":"code","source":["# submit a metadata request\n","get_scanner_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?reportObjects=True&getTridentArtifacts=True&getArtifactUsers=true&lineage=True&datasourceDetails=True&datasetSchema=True&datasetExpressions=True'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","list_of_workspaces=f\"{{'workspaces' : [ '{demo_workspace_id}']}}\"\n","#print(\"list=\", list_of_workspaces)\n","\n","response=requests.post(get_scanner_url,headers=header_token, data=list_of_workspaces)\n","print(\"response=\",response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27b8620a-a192-4ce4-be8e-5988ea7d7e1f"},{"cell_type":"code","source":["# check status for the metadata request\n","#GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanId}\n","\n","parsable=json.loads(response.text)\n","scanid=response.json().get('id')\n","print(\"scanid=\",scanid)\n","\n","get_scanstatus_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanid}'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","response=requests.get(get_scanstatus_url,headers=header_token)\n","print(\"response=\",response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fb3f226-9c31-4404-9a45-b53c2ce04f40"},{"cell_type":"markdown","source":["Finally we get the result of the Metadata Scanning API request. The output can be quite bulky, so I suggest to explore it via a good JSON editor or specific json queries."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad194116-40cc-4e6b-b153-ac3356b1f1ba"},{"cell_type":"code","source":["#finally get metadata\n","#GET https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanId}\n","\n","get_scanresult_url = f'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanid}'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","\n","workspaces_response=requests.get(get_scanresult_url,headers=header_token)\n","print(\"workspaces_response\",workspaces_response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ac98db01-c133-4da5-8e60-dab0e2e76b42"},{"cell_type":"markdown","source":["# **EXAMPLE: METADATA EXTRACTION USING SQL**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"308aa746-663f-451e-ab98-b6f2238f966b"},{"cell_type":"code","source":["#EXTRACT TABLES USING SQL\n","# this is provided just as an example: it might give errors but you can ignore this cell \n","df2 = spark.sql(\"SELECT name, object_id, create_date, modify_date, type, type_desc from sys.tables\")\n","display(df2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f02dd016-026e-4ed1-91ef-cccd2dad7ae0"},{"cell_type":"markdown","source":["# **EXAMPLE: SEMANTIC LINK LAB LIBRARY**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44d2cb68-1256-4e3f-8a0a-5c1a7fb4e6ab"},{"cell_type":"code","source":["# Example Use of Semantic Link Labs\n","# For checking reports\n","#\n","# this is just an example/demo cell and does not interfere with the metadata extraction in the following cells\n","# you can skip it \n","report_name = '<ENTER THE NAME OF A PBIR REPORT HERE>' # Enter the report name\n","report_workspace = demo_workspace # Enter the workspace in which the report exists\n","rpt = ReportWrapper(report=report_name, workspace=report_workspace)\n","\n","rpt.list_semantic_model_objects()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:03:32.3578244Z","session_start_time":null,"execution_start_time":"2025-05-05T15:03:32.3590052Z","execution_finish_time":"2025-05-05T15:03:37.0364663Z","parent_msg_id":"7e56a514-76db-4f3d-ac29-e9d31f018f5d"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"        Table Name  Object Name Object Type Report Source  \\\n0        GoldTable    ProductID      Column        Visual   \n1        GoldTable      Revenue      Column        Visual   \n2  SalesByCustomer  CompanyName      Column        Visual   \n3  SalesByCustomer      Vendite      Column        Visual   \n\n             Report Source Object  \n0  'Page 1'[61f729cc45bb810be09c]  \n1  'Page 1'[61f729cc45bb810be09c]  \n2  'Page 1'[e3d843090ce4633b997b]  \n3  'Page 1'[e3d843090ce4633b997b]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Table Name</th>\n      <th>Object Name</th>\n      <th>Object Type</th>\n      <th>Report Source</th>\n      <th>Report Source Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GoldTable</td>\n      <td>ProductID</td>\n      <td>Column</td>\n      <td>Visual</td>\n      <td>'Page 1'[61f729cc45bb810be09c]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GoldTable</td>\n      <td>Revenue</td>\n      <td>Column</td>\n      <td>Visual</td>\n      <td>'Page 1'[61f729cc45bb810be09c]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SalesByCustomer</td>\n      <td>CompanyName</td>\n      <td>Column</td>\n      <td>Visual</td>\n      <td>'Page 1'[e3d843090ce4633b997b]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SalesByCustomer</td>\n      <td>Vendite</td>\n      <td>Column</td>\n      <td>Visual</td>\n      <td>'Page 1'[e3d843090ce4633b997b]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"985f0ec5-80a5-4957-9674-081b6fb6a08d"},{"cell_type":"markdown","source":["# **START OF MAIN PROCESS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc5ebfe7-3731-4c83-bd98-3bc0228eff5f"},{"cell_type":"markdown","source":["# **EXTRACT ALL WORKSPACES METADATA**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"350e2802-d322-4e3c-8e44-e1cc9583b09a"},{"cell_type":"code","source":["#this is the real extraction process\n","#get workspaces\n","get_workspaces_url = f'https://api.fabric.microsoft.com/v1/workspaces'\n","header_token={\"Authorization\": bearer, 'Content-Type': 'application/json'}\n","data_dummy={\"dummy\":\"true\"}\n","#print(\"header_token=\", header_token)\n","workspaces_response=requests.get(get_workspaces_url,headers=header_token, data=data_dummy)\n","print(\"workspaces_response\",workspaces_response.text)\n","\n","\n","wkspaces=json.loads(workspaces_response.text)[\"value\"]\n","for wkspc in wkspaces:\n","    \n","    current_wkspace_id=wkspc[\"id\"]\n","    current_wkspace_name=wkspc[\"displayName\"]\n","    ##hack for demo: if you need to just display one workspace (for test or demo) uncomment these 2 lines\n","    if current_wkspace_name != demo_workspace:\n","      continue\n","    \n","    print(f'Processing workspace:  {current_wkspace_name} ({current_wkspace_id})\\n')\n","    \n","    new_row = df_tables_datarow.copy()\n","    new_row.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id})\n","    df_tables = df_tables.union(spark.createDataFrame(data=[new_row],schema=schema_df_tables))\n","    df_tables.show()\n","    \n","\n","    get_items_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/items'\n","    items_response=requests.get(get_items_url,headers=header_token, data=data_dummy)\n","    #print(items_response.text)\n","  \n","    items=json.loads(items_response.text)[\"value\"]\n","    for item in items:\n","      item_name=item[\"displayName\"]\n","      item_type=item[\"type\"]\n","      item_id=item[\"id\"]\n","      print(f'\\n{item_name} is of type {item_type} and has id={item[\"id\"]}')\n","      if item_type == \"Warehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          # get the SERVER part of the SQL connection string\n","          # https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses/{warehouseId} \n","          get_SQLString_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/warehouses/{item_id}'\n","          sql_url_response=requests.get(get_SQLString_url,headers=header_token, data=data_dummy)\n","          fabric_SQL_connection_string=json.loads(sql_url_response.text)[\"properties\"][\"connectionString\"]\n","          print(f\"SQLconn_string for {item_name} is {fabric_SQL_connection_string}\")\n","          odbc_conn_string=f'{odbc_connection_string_template}SERVER={fabric_SQL_connection_string};DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute a query\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              #print(\"Found table: \",row)\n","              table_name=row[0]\n","\n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"WarehouseName\": f\"{database_name}\", \\\n","              \"WarehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"WarehouseName\": f\"{database_name}\", \\\n","                \"WarehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","              \n","          #close correctly\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Lakehouse\":\n","          print(f\"Exploring {item_name}\")\n","          database_name=item_name\n","          # get the SERVER part of the SQL connection string\n","          # https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses/{warehouseId} \n","          get_SQLString_url = f'https://api.fabric.microsoft.com/v1/workspaces/{current_wkspace_id}/lakehouses/{item_id}'\n","          sql_url_response=requests.get(get_SQLString_url,headers=header_token, data=data_dummy)\n","          fabric_SQL_connection_string=json.loads(sql_url_response.text)[\"properties\"][\"sqlEndpointProperties\"][\"connectionString\"]\n","          print(f\"SQLconn_string for {item_name} is {fabric_SQL_connection_string}\")\n","          odbc_conn_string=f'{odbc_connection_string_template}SERVER={fabric_SQL_connection_string};DATABASE={database_name}'\n","          conn = pyodbc.connect(odbc_conn_string)\n","          # Execute a query\n","          cursor = conn.cursor()\n","          cursor.execute(query_tables)\n","          tableList = cursor.fetchall()\n","          table_name=\"\"\n","          for row in tableList:\n","              print(\"Found table: \",row)\n","              table_name=row[0]\n","              \n","              # save table name in dataframe for tables\n","              new_dict=df_tables_datarow.copy()\n","              new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","              \"LakehouseName\": f\"{database_name}\", \\\n","              \"LakehouseID\": f\"{item_id}\", \\\n","              \"TableName\": f'{table_name}', \\\n","              \"TableType\": 'Table', \\\n","              \"TableID\": f'{row[1]}'})\n","              #print(f'new_dict={new_dict}')\n","              new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","              #print(f'new_dict={new_row}')\n","              df_tables = df_tables.union(new_row)\n","              #df_tables.show()\n","\n","              #extract  details of table columns\n","              #\n","              \n","              query_columns=f'{query_columns_template1}{table_name}{query_columns_template2}'\n","              #print(query_columns,\"\\n\")\n","              cursor.execute(query_columns)\n","              columnList=cursor.fetchall()\n","              for column in columnList:\n","                # extract  column_name,column_id,type_name\n","                # save column name in dataframe for columns\n","                new_dict=df_columns_datarow.copy()\n","                new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                \"LakehouseName\": f\"{database_name}\", \\\n","                \"LakehouseID\": f\"{item_id}\", \\\n","                \"TableName\": f'{table_name}', \\\n","                \"TableID\": f'{row[1]}', \\\n","                \"ColumnName\": f'{column[0]}', \\\n","                \"ColumnID\": f'{column[1]}', \\\n","                \"ColumnType\": f'{column[3]}'})\n","                #print(f'new_dict={new_dict}')\n","                new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_columns)\n","                #print(f'new_dict={new_row}')\n","                df_columns = df_columns.union(new_row)\n","          #close correctly\n","          cursor.close()\n","          conn.close()\n","          \n","\n","      if item_type == \"Report\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit for reports\n","          try:\n","                rpt = ReportWrapper(report=item_name, workspace=current_wkspace_name)\n","                pandadf_report_lineage=rpt.list_semantic_model_objects()\n","                #print(pandadf_report_lineage)\n","                for row in pandadf_report_lineage.itertuples():\n","                    #print(\"pandas row=\",row)\n","                    #print(row[2])\n","                    if row[3] == \"Column\":\n","                        #\n","                        # save report lineage item in dataframe for reports\n","                        new_dict=df_reports_datarow.copy()\n","                        new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                        \"ReportName\": f'{item_name}', \\\n","                        \"ReportID\": f'{item_id}', \\\n","                        \"ColumnName\": f'{row[2]}', \\\n","                        \"SourceName\": f'{row[1]}', \\\n","                        #\"Source ID\": f'{column[1]}', \\\n","                        \"SourceType\": \"Table\"}) #only tables so far\n","                        #print(f'new_dict={new_dict}')\n","                        new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_reports)\n","                        #print(f'new_dict={new_row}')\n","                        df_reports = df_reports.union(new_row)\n","                        #\n","          except ValueError as verr:\n","            print(\"report not valid\")\n","            print(\"error=\",verr)\n","          except Fabric as httpex:\n","            print(\"report not valid\")\n","            print(\"error=\",httpex)\n","\n","                \n","     \n","      if item_type == \"DataPipeline\":\n","          print(f\"Exploring {item_name}\")\n","          # use semantic toolkit for pipelines\n","          pipeline_definition=labs.get_data_pipeline_definition(item_name,current_wkspace_id,decode=True)\n","          # check which type, and if we can manage this kind of pipeline\n","          # only pipelines with copy activity are managed so far\n","          all_activities=json.loads(pipeline_definition)[\"properties\"][\"activities\"]\n","          try:\n","                for activity in all_activities:\n","                        #print(\"activity \",activity)\n","                        activity_type=activity[\"type\"]\n","                        if activity_type == \"Copy\":\n","                            #gather metadata\n","                            activity_name=activity[\"name\"]\n","                            source_type = activity[\"typeProperties\"][\"source\"][\"type\"]\n","                            source_container=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            source_container_id=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            source_name=activity[\"typeProperties\"][\"source\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            sink_type=activity[\"typeProperties\"][\"sink\"][\"type\"]\n","                            sink_container=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"name\"]\n","                            sink_container_id=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"linkedService\"][\"properties\"][\"typeProperties\"][\"artifactId\"]\n","                            sink_name=activity[\"typeProperties\"][\"sink\"][\"datasetSettings\"][\"typeProperties\"][\"table\"]\n","                            source_sink_mappings=activity[\"typeProperties\"][\"translator\"][\"mappings\"]\n","                            mappings_list=[]\n","                            for mapping_item in source_sink_mappings:\n","                                mappings_list.append({\"Source\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"type\"]} {mapping_item[\"source\"][\"physicalType\"]})', \\\n","                                \"Sink\": f'{mapping_item[\"source\"][\"name\"]} ({mapping_item[\"source\"][\"physicalType\"]})'})\n","                            mappings_string=json.dumps(mappings_list)\n","                            print(\"mappings to string\",mappings_string)\n","                            # save metadata in dataframe\n","                            new_dict=df_lineage_datarow.copy()\n","                            new_dict.update({\"WorkspaceName\": current_wkspace_name, \"WorkspaceID\": current_wkspace_id, \\\n","                            \"PipelineName\": item_name, \\\n","                            \"PipelineID\": item_id, \\\n","                            \"ActivityName\": activity_name, \\\n","                            \"ActivityType\": activity_type, \\\n","                            \"SourceName\": source_name, \\\n","                            \"SourceType\": source_type, \\\n","                            \"SourceContainerName\": source_container, \\\n","                            \"SourceContainerID\": source_container_id, \\\n","                            \"SinkType\": sink_type, \\\n","                            \"SinkName\": sink_name, \\\n","                            \"SinkContainerName\": sink_container, \\\n","                            \"SinkContainerID\": sink_container_id, \\\n","                            \"ColumnMappings\": f'{mappings_string}', \\\n","                            \"PurviewFQN\":\"\"})\n","                            \n","                            #print(f'new_dict={new_dict}')\n","                            new_row = spark.createDataFrame(data=[new_dict],schema=schema_df_lineage)\n","                            #print(f'new_dict={new_row}')\n","                            df_lineage = df_lineage.union(new_row)\n","                            break\n","          except KeyError as keyerr:\n","            print(\"Error parsing DataPipelines definition: \", keyerr)\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"error details \", keyerr)\n","          except NameError as namerr:\n","            #print(\"error parsing activities: \",all_activities)\n","            print(\"Error parsing DataPipelines definition: \", namerr)\n","                    \n","\n","              \n","              \n","print(\"Recap of Metadata found:\")\n","\n","print(\"Found Tables (includes partial rows for Lakehouses and Warehouses):\")\n","display(df_tables)\n","#df_tables.show()\n","\n","print(\"\\n\\nFound Table columns (for all tables) found:\")\n","#df_columns.show()\n","display(df_columns)\n","\n","print(\"\\n\\nFound PBIR Reports lineage to tables/views:\")\n","display(df_reports)\n","#df_reports.show()\n","\n","print(\"\\n\\nFound Copy DataPipelines, with lineage to tables/views:\")\n","display(df_lineage)\n","#df_lineage.show()\n","\n","print(\"\\nMetadata Extraction Completed.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f3709664-6259-4004-97b2-f0efb7fd7c50"},{"cell_type":"code","source":["import json\n","import pyapacheatlas\n","# PyApacheAtlas packages\n","# Connect to Atlas via a Service Principal\n","from pyapacheatlas.auth import ServicePrincipalAuthentication\n","from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess\n","from pyapacheatlas.core.typedef import EntityTypeDef, AtlasAttributeDef\n","from pyapacheatlas.core.util import GuidTracker\n","\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, when\n","import pyspark.sql"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:06:39.3786768Z","session_start_time":null,"execution_start_time":"2025-05-05T15:06:39.37988Z","execution_finish_time":"2025-05-05T15:06:39.6653368Z","parent_msg_id":"eab621b8-e97e-4dad-8eff-5bbae7b84aa1"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 26, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4465fa93-6736-46e4-ae7f-11856f3225c6"},{"cell_type":"code","source":["# here we enter our Service principal id and secret to access the MS Purview account APIs\n","oauth = ServicePrincipalAuthentication(\n","        tenant_id=tenantid4purview,\n","        client_id=clientid4purview,\n","        client_secret=clientsecret4purview\n","    )\n","client = PurviewClient(\n","        account_name=PurviewAccount_name,\n","        authentication=oauth\n","    )\n","    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:06:43.799135Z","session_start_time":null,"execution_start_time":"2025-05-05T15:06:43.8002664Z","execution_finish_time":"2025-05-05T15:06:44.0971324Z","parent_msg_id":"56a65443-6b7a-4286-ab20-8cb6a05197bb"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 27, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70fd7857-43db-446f-a95a-e612e89edc87"},{"cell_type":"code","source":[" # We need a custom process entity type that contains the definition for\n","    # a columnMapping attribute.\n","# THIS SHOULD ONLY BE RUN ONCE\n","procType = EntityTypeDef(\n","    \"Lineage_Extractor_Process\",\n","    superTypes=[\"Process\"],\n","    attributeDefs = [\n","        AtlasAttributeDef(\"columnMapping\")\n","    ]\n",")\n","\n","# Upload the type definition\n","type_results = client.upload_typedefs(entityDefs=[procType], force_update=True)\n","#print(json.dumps(type_results,indent=2))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"16f63d8f-a1ac-4875-9731-887abccedfa4","normalized_state":"finished","queued_time":"2025-05-05T15:06:50.8153961Z","session_start_time":null,"execution_start_time":"2025-05-05T15:06:50.8164659Z","execution_finish_time":"2025-05-05T15:06:54.1809526Z","parent_msg_id":"b747d9f4-ce7f-471a-a536-3e386b551874"},"text/plain":"StatementMeta(, 16f63d8f-a1ac-4875-9731-887abccedfa4, 28, Finished, Available, Finished)"},"metadata":{}}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"57604287-caa9-4c8c-9cdb-8f43d9ec21b7"},{"cell_type":"markdown","source":["# **UPLOAD TABLES to PURVIEW DATA GOVERNANCE**\n","\n","code for upload is taken from Pyapacheatlas samples by Will Johnson ( https://github.com/wjohnson/pyapacheatlas )"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71999375-cd28-4380-8179-0268dce09cd6"},{"cell_type":"code","source":["#new df where we will update the Purview fullyQualifiedName and GUID\n","df_tables_registered=spark.createDataFrame(data=[],schema=schema_df_tables)\n","\n","created_GUIDs=[]\n","# create Table nodes\n","gt = GuidTracker()\n","for table in df_tables.collect():\n","    table_dict=table.asDict()\n","    #print(\"table_dict=\",table_dict)\n","    tablename=table_dict[\"TableName\"]\n","    \n","    if tablename:\n","        if table_dict[\"WarehouseName\"]:\n","            artifactName=table_dict[\"WarehouseName\"]\n","            artifactType=\"warehouse\"\n","            artifactID=table_dict[\"WarehouseID\"]\n","        else:\n","            artifactName=table_dict[\"LakehouseName\"]\n","            artifactType=\"lakehouse\"\n","            artifactID=table_dict[\"LakehouseID\"]\n","        table_entity = AtlasEntity(\n","            name=tablename,\n","            typeName=\"DataSet\",\n","            qualified_name=f'LineageExtractor://workspace/{table_dict[\"WorkspaceID\"]}/{artifactType}/{artifactID}/table/{table_dict[\"TableID\"]}',\n","            guid=gt.get_guid(),\n","            attributes={\n","                \"userDescription\": f'<div>Fabric Table<p> {tablename} is a Table contained within {artifactType} {artifactName} in Workspace {table_dict[\"WorkspaceName\"]}</p>'\n","            },\n","            # This custom attribute flips a switch inside of the Purview UI to render\n","            # the rich text description.\n","            customAttributes={\n","                \"microsoft_isDescriptionRichText\": \"true\"\n","            }\n","        )\n","        #print(table_entity)\n","        #print(table_entity.guid)\n","        #print(table_entity.qualifiedName)\n","        results = client.upload_entities([table_entity])\n","        \n","        table_dict[\"PurviewGUID\"] = results[\"guidAssignments\"][str(table_entity.guid)]    \n","        table_dict[\"PurviewFQN\"] = str(table_entity.qualifiedName)\n","        #print(f'new guid, new fqn={table_dict[\"PurviewGUID\"]}, {table_dict[\"PurviewFQN\"]}')\n","        \n","        created_GUIDs.append(table_dict[\"PurviewGUID\"])\n","\n","        new_dict=df_tables_datarow.copy()\n","        for key in table_dict.keys():\n","            #print(\"key=\",key)\n","            new_dict[key]=table_dict[key]\n","        #print(\"new_dict=\",new_dict)\n","        new_row=spark.createDataFrame(data=[new_dict],schema=schema_df_tables)\n","        \n","        df_tables_registered=df_tables_registered.union(new_row)\n","          \n","        #df_tables.union(table)\n","        #df_tables=df_tables.withColumn(\"PurviewGUID\", when(col(\"TableName\") == tablename, createdPurviewGUID))\n","        #df_tables=df_tables.withColumn(\"PurviewFQN\", when(col(\"TableName\") == tablename, createdPurviewFQN))\n","\n","display(df_tables_registered)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b54b3937-e95f-4623-8bf9-3df0847f2d37"},{"cell_type":"markdown","source":["# **UPLOAD REPORTS AND THEIR LINEAGE**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a2b446ff-5da1-4c04-9211-c7e857946ba3"},{"cell_type":"code","source":["#new df where we will update the Purview fullyQualifiedName and GUID\n","df_reports_registered=df_reports.alias(\"df_reports_registered\")\n","#generates an empty copy of the Dataframe\n","\n","\n","gt = GuidTracker()\n","created_GUIDs=[]\n","# create Reports nodes and lineage\n","#first find all detected reports IDs\n","report_distinctIds=df_reports.select('ReportID').distinct().collect()\n","#print(\"\\nreport_distinctIds:\")\n","#display(report_distinctIds)\n","#print()\n","\n","\n","for report_id in report_distinctIds:\n","    rep_id_dict=report_id.asDict()\n","    #print(\"\\nrep_id_dict\",rep_id_dict)\n","    current_report_id=rep_id_dict[\"ReportID\"]\n","    report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","    report_sources=report_rows.collect()\n","    #print(\"report sources=\",report_sources)\n","    #print()\n","\n","    #for each report register it, and create lineage with tables\n","    #first register it if not already scanned by Purview\n","    report_metadata=report_sources[0].asDict()\n","    report_guid=report_metadata[\"PurviewGUID\"]\n","    report_fqn=report_metadata[\"PurviewFQN\"]\n","    if report_guid == \"\":\n","        report_entity = AtlasEntity(\n","            name=report_metadata[\"ReportName\"],\n","            typeName=\"DataSet\",\n","            qualified_name=f'LineageExtractor://workspace/{report_metadata[\"WorkspaceID\"]}/report/{current_report_id}',\n","            guid=gt.get_guid(),\n","            attributes={\n","                \"userDescription\": f'<div>Fabric Report<p>in PBIR format</p><p>contained within {report_metadata[\"WorkspaceName\"]}</p><p>Fabric ID is {current_report_id}</p>'\n","            },\n","            # This custom attribute flips a switch inside of the Purview UI to render\n","            # the rich text description.\n","            customAttributes={\n","                \"microsoft_isDescriptionRichText\": \"true\"\n","            }\n","        )\n","        #print(report_entity)\n","        #print(report_entity.guid)\n","        #print(report_entity.qualifiedName)\n","        results = client.upload_entities([report_entity])\n","        #print(\"\\nrisultato upload=\",json.dumps(results))\n","\n","        report_guid = results[\"guidAssignments\"][str(report_entity.guid)]    \n","        report_fqn = str(report_entity.qualifiedName)\n","        #print(f'\\nnew guid, new fqn={report_guid}, {report_fqn}')\n","        \n","        created_GUIDs.append(report_guid)\n","        #change all row occurrences of the report by updating FQN and GUID\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewGUID\", when(col(\"ReportID\") == current_report_id, report_guid).otherwise(col(\"PurviewGUID\")))\n","        df_reports_registered=df_reports_registered.withColumn(\"PurviewFQN\", when(col(\"ReportID\") == current_report_id, report_fqn).otherwise(col(\"PurviewFQN\")))       \n","        #refresh report_rows\n","        report_rows=df_reports_registered.filter(col(\"ReportID\")==current_report_id)\n","        \n","    #we use a join between the source table name here in the df of reports and the df of tables, then for each row we create a lineage link\n","    reports_short=report_rows.select('ReportID','SourceName',\"ColumnName\",\"PurviewFQN\").withColumnRenamed(\"PurviewFQN\",\"ReportFQN\")\n","    #print(\"\\nreports_short:\")\n","    #print(reports_short)\n","    #display(reports_short)\n","    #print()\n","\n","    lineage_links=reports_short.join(df_tables_registered.select(\"TableName\",\"PurviewFQN\"),reports_short[\"SourceName\"] == df_tables_registered[\"TableName\"],\"inner\").orderBy(\"TableName\").collect()\n","    #print(\"\\nlinks=\",lineage_links)\n","    #display(lineage_links)\n","\n","    # loop through source tables to create lineage columns mappings\n","    column_mapping_total=[]\n","    column_mapping_partial=[]\n","    inputs=[]\n","    sourceFQN=\"\"\n","    sinkFQN=\"\"\n","    current_table=lineage_links[0][\"TableName\"]\n","    previous_table=current_table\n","    #print(f'\\nprevious table={previous_table}, current table={current_table}')\n","    for link in lineage_links:\n","        link_dict=link.asDict()\n","        #print(\"\\nlink_dict=\",link_dict)\n","        current_table=link_dict[\"TableName\"]\n","        if previous_table == current_table:\n","            # just add another mapping entry\n","            #print(\"column in same table as previous loop\")\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","        else:\n","            # pack info for current table\n","            #print(f'\\nprevious table={previous_table}, current table={current_table}')\n","            #print(f'different tables from previous loop')\n","\n","            column_mapping_total.append(\n","                # This object defines the column lineage between table souces and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","            column_mapping_partial=[]\n","            registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","            #print(\"found table entity =\",registered_table_entity)\n","            inputs.append(registered_table_entity)\n","            # take new values\n","            columnname=link_dict[\"ColumnName\"]\n","            column_mapping_partial.append({\"Source\": columnname,\"Sink\":\"Visual\"})\n","            sourceFQN=link_dict[\"PurviewFQN\"]\n","            sinkFQN=link_dict[\"ReportFQN\"]\n","            previous_table=current_table\n","    # process and close last table column\n","    #print(\"\\nlast\")\n","    registered_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sourceFQN)[\"entities\"][0]\n","    #print(\"found table entity =\",registered_table_entity)\n","    inputs.append(registered_table_entity)\n","    # take new values\n","    columnname=link_dict[\"ColumnName\"]\n","    #column_mapping_partial.append({\"Source:\": columnname,\"Sink:\":\"Visual\"})\n","    \n","    column_mapping_total.append(\n","                # This object defines the column lineage between table sources and the current report\n","                {\"ColumnMapping\": column_mapping_partial,\n","                \"DatasetMapping\": {\n","                        \"Source\": sourceFQN, \"Sink\": sinkFQN}\n","                }\n","            )\n","\n","    registered_report_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=report_fqn)[\"entities\"][0]\n","\n","    print('\\n\\n\\ncolumn_mapping_total=', json.dumps(column_mapping_total))\n","    lineage_link_obj = AtlasProcess(\n","        name=f'Report Lineage for {report_metadata[\"ReportName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'LineageExtractor://workspace/{report_metadata[\"WorkspaceID\"]}/report/{current_report_id}/ReportLineage',\n","        inputs=inputs,\n","        outputs=[registered_report_entity],\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric Report Mapping<p> for Report {report_metadata[\"ReportName\"]} contained within workspace {report_metadata[\"WorkspaceName\"]}</p> <p> Report Id is {current_report_id}</p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    #print(\"\\ncreated guids=\",created_GUIDs)\n","    #print(\"\\nentity to be created=\",lineage_link_obj)\n","    results = client.upload_entities([lineage_link_obj])\n","    #print(\"\\n upload result=\",json.dumps(results))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9fbbcf6d-0dac-48df-b0b7-dc19a2d39aa6"},{"cell_type":"markdown","source":["# **UPLOAD PIPELINES AND THEIR LINEAGE**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2be18311-7afa-4b6b-8046-d12f0ef67d89"},{"cell_type":"code","source":["df_pipelines_registered=df_lineage.alias(\"df_pipelines_registered\")\n","\n","gt = GuidTracker()\n","created_GUIDs=[]\n","# create Reports nodes and lineage\n","#first find all found reports IDs\n","pipelines_list=df_pipelines_registered.collect()\n","print(\"\\npipelines_list:\")\n","display(pipelines_list)\n","print()\n","\n","\n","for pipeline in pipelines_list:\n","    source_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SourceName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SourceContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SourceContainerID\"]))).first()\n","    source_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=source_table[\"PurviewFQN\"])[\"entities\"][0]\n","    #print(\"found source table entity =\",source_table_entity)\n","    inputs=[source_table_entity]\n","\n","    sink_table=df_tables_registered.filter((col(\"TableName\")== pipeline[\"SinkName\"]) & (col(\"WorkspaceID\")==pipeline[\"WorkspaceID\"]) & ((col(\"LakehouseID\")==pipeline[\"SinkContainerID\"]) | (col(\"WarehouseID\")==pipeline[\"SinkContainerID\"]))).first()\n","    sink_table_entity = client.get_entity(typeName=\"DataSet\",qualifiedName=sink_table[\"PurviewFQN\"])[\"entities\"][0]\n","    #print(\"found sink table entity =\",sink_table_entity)\n","    outputs=[sink_table_entity]\n","    \n","    column_mapping_partial=json.loads(pipeline[\"ColumnMappings\"])\n","    column_mapping_total=[{\"ColumnMapping\": column_mapping_partial,\n","        \"DatasetMapping\": {\"Source\": source_table[\"PurviewFQN\"], \"Sink\": sink_table[\"PurviewFQN\"]}\n","        }]\n","\n","\n","    pipeline_entity=AtlasProcess(\n","        name=f'DataPipeline {pipeline[\"PipelineName\"]}',\n","        typeName=\"Lineage_Extractor_Process\",\n","        qualified_name=f'DataPipeline://workspace/{pipeline[\"WorkspaceID\"]}/pipeline/{pipeline[\"PipelineID\"]}/CopyActivity',\n","        inputs=inputs,\n","        outputs=outputs,\n","        guid=gt.get_guid(),\n","        attributes={\n","            \"columnMapping\": json.dumps(column_mapping_total),\n","            \"userDescription\": f'<div>Fabric DataPipeline Mapping<p> contained within {pipeline[\"WorkspaceName\"]}</p> <p> </p>'\n","        },\n","        # This custom attribute flips a switch inside of the Purview UI to render\n","        # the rich text description.\n","        customAttributes={\n","            \"microsoft_isDescriptionRichText\": \"true\"\n","        }\n","    )\n","    results = client.upload_entities([pipeline_entity])\n","    #print(results)\n","    #print(\"\\n upload result=\",json.dumps(results))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8e989797-cc57-4237-86c7-5b0ef5b720c7"},{"cell_type":"markdown","source":["# **END OF PROCESS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2533b769-2a88-4260-a0f4-fedc7e2b3b34"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"7200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"37cdc82e-9d7c-4406-8670-6bf5dbeacb66","default_lakehouse_name":"lakehouse_for_lineage","default_lakehouse_workspace_id":"6f587540-db76-497f-acf8-170bc6d88580"}}},"nbformat":4,"nbformat_minor":5}